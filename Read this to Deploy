# Specification

| Server        | CPU   | RAM  | Function | IP    |  OS   |
| ------------- | ----- | ---- | -------- | --- | --- |
| kube-master   | 2 CPU | 2GB  | Open5GS  | 192.168.1.5    |   Ubuntu Server 24.04  |
| kube-worker-1 | 2 CPU | 2 GB | EURANSIM |  192.168.1.6   |  Ubuntu Server 24.04   |

# Referensi
https://luislogs.com/posts/security-and-observability-with-cilium-on-my-5g-network/

# Pre-requisite
1. Install kubernetes, helm (urusan masing-masing yah)
2. install cilium CLI
```
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ &#34;$(uname -m)&#34; = &#34;aarch64&#34; ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
sha256sum --check cilium-linux-${CLI_ARCH}.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-${CLI_ARCH}.tar.gz /usr/local/bin
rm cilium-linux-${CLI_ARCH}.tar.gz{,.sha256sum}
```
3. install cilium
```
helm repo add cilium https://helm.cilium.io/
helm repo update
helm install cilium cilium/cilium --namespace kube-system
```
tunggu sampai berhasil inisialisasi, cek lewat:
```
kubectl get daemonset -n kube-system
```
4. tambah repo helm kube prometheus stack
```
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
```
5. buat values.yml
```
grafana:
  enabled: true

prometheus:
  enabled: true
  prometheusSpec:
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: prometheus-server
          accessModes: [&#34;ReadWriteOnce&#34;]
          resources:
            requests:
              storage: 5Gi
    additionalScrapeConfigsSecret:
      enabled: true
      name: cilium-hubble-scrape-config
      key: prometheus.yml

    securityContext:
      fsGroup: 1000
      runAsUser: 65534
      runAsNonRoot: true

alertmanager:
  enabled: true

# Other components can be configured here as well (kube-state-metrics, node-exporter, etc.)
# For example, if you don&#39;t need node-exporter on every node:
# prometheus-node-exporter:
#   enabled: true

# kube-state-metrics:
#  enabled: true
```
6. buat prometheus.yml
```
scrape_configs:
  # This job discovers pods based on annotations.
  # I&#39;ve renamed it to &#39;custom-kubernetes-pods&#39; to avoid potential conflicts
  # with default jobs that kube-prometheus-stack might configure.
  # Your original name was &#39;kubernetes-pods&#39;.
  - job_name: &#39;custom-kubernetes-pods&#39;
    kubernetes_sd_configs:
      - role: pod
        # If this scrape config is ONLY for Cilium/Hubble pods in &#39;kube-system&#39;,
        # you might want to restrict the discovery to that namespace:
        # namespaces:
        #   names:
        #   - kube-system
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: ${1}:${2}
        target_label: __address__

  # This job discovers services/endpoints based on annotations.
  # I&#39;ve renamed it to &#39;custom-kubernetes-endpoints&#39;.
  # Your original name was &#39;kubernetes-endpoints&#39;.
  - job_name: &#39;custom-kubernetes-endpoints&#39;
    scrape_interval: 30s
    kubernetes_sd_configs:
      - role: endpoints
        # If this scrape config is ONLY for Cilium/Hubble services in &#39;kube-system&#39;,
        # you might want to restrict the discovery to that namespace:
        # namespaces:
        #   names:
        #   - kube-system
    relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: (.+)(?::\d+);(\d+)
        replacement: $1:$2
```
7. buat secret utk prometheus
nano prometheus-additional-scrape-secret.yaml
```
apiVersion: v1
kind: Secret
metadata:
  name: cilium-hubble-scrape-config # This name must match what&#39;s in your values.yaml
  namespace: monitoring             # Or your target namespace for kube-prometheus-stack
type: Opaque
stringData:
  prometheus.yml: |                 # This is the key
    scrape_configs:
      - job_name: &#39;kubernetes-pods&#39;
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: ${1}:${2}
            target_label: __address__

      - job_name: &#39;kubernetes-endpoints&#39;
        scrape_interval: 30s
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
            action: replace
            target_label: __address__
            regex: (.+)(?::\d+);(\d+)
            replacement: $1:$2
```
8. buat presistent volume
nano kube-prometheus-stack-pv.yaml
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: kube-prometheus-stack-pv
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/data/prometheus
```
nano kube-prometheus-stack-pvc.yaml
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kube-prometheus-stack-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  volumeName: kube-prometheus-stack-pv
```

apply yamlnya:
```
kubectl apply -f kube-prometheus-stack-pv.yaml
kubectl apply -f kube-prometheus-stack-pvc.yaml

kubectl apply -f prometheus-additional-scrape-secret.yaml -n monitoring

kubectl delete secret kube-prometheus-stack-admission -n monitoring --ignore-not-found=true

helm install kube-prometheus-stack prometheus-community/kube-prometheus-stack -n monitoring -f values.yml
```
cek pass grafana:
```
kubectl get secret --namespace monitoring kube-prometheus-stack-grafana -o jsonpath=&#34;{.data.admin-password}&#34; | base64 --decode ; echo
```
terus tinggal:
```
sudo chown -R 65534:65534 /mnt/data/prometheus
```
9. enable promethues &amp; grafana di cilium
```
helm upgrade cilium cilium/cilium \
  --namespace kube-system \
  --set prometheus.enabled=true \
  --set operator.prometheus.enabled=true
```
10. enable hubble di cilium
```
helm upgrade cilium cilium/cilium --namespace kube-system \
   --reuse-values \
   --set hubble.relay.enabled=true \
   --set hubble.ui.enabled=true \
   --set hubble.ui.frontend.server.ipv6.enabled=false
   --set hubble.enabled=true \
   --set hubble.metrics.enableOpenMetrics=true \
   --set hubble.metrics.enabled=&#34;{dns,drop,tcp,flow,port-distribution,icmp,httpV2:exemplars=true;labelsContext=source_ip\,source_namespace\,source_workload\,destination_ip\,destination_namespace\,destination_workload\,traffic_direction}&#34;
```
cek hubble lewat:
```
cilium status --wait
```
11. install hubble client
```
HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)
HUBBLE_ARCH=amd64
if [ &#34;$(uname -m)&#34; = &#34;aarch64&#34; ]; then HUBBLE_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}
sha256sum --check hubble-linux-${HUBBLE_ARCH}.tar.gz.sha256sum
sudo tar xzvfC hubble-linux-${HUBBLE_ARCH}.tar.gz /usr/local/bin
rm hubble-linux-${HUBBLE_ARCH}.tar.gz{,.sha256sum}
```
cek hubble lewat:
```
hubble version
```
12. Akses Prometheus &amp; grafana
prometheus:
```
kubectl port-forward --address 0.0.0.0 svc/kube-prometheus-stack-prometheus 9090:9090 -n monitoring
```
grafana:
```
kubectl port-forward --address 0.0.0.0 svc/kube-prometheus-stack-grafana 3000:80 -n monitoring
```

# Open5GS with Cilium
1. bikin hubble bisa monitor packet
buat file yaml `l4-egress-to-dns.yaml`
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-egress-to-dns&#34;
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/instance: open5gs
  egress:
    - toPorts:
        - ports:
            - port: &#34;53&#34;
              protocol: ANY
          rules:
            dns:
              - matchPattern: &#34;*&#34;
```
apply pake:
```
kubectl apply -f l4-egress-to-dns.yaml
```
port forward:
```
cilium hubble port-forward&amp;
```
## Open5gs
1. buat yaml
bikin file open5gs.yaml:
```
hss:
  enabled: false

# SCP is enabled and gets the SBI label
scp:
  enabled: true
  podLabels:
    sbi: enabled

mme:
  enabled: false

pcrf:
  enabled: false

# PCF is enabled and gets the SBI label
pcf:
  enabled: true
  podLabels:
    sbi: enabled
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: kube-prometheus-stack

# SMF is enabled and gets the SBI label
smf:
  enabled: true
  podLabels:
    sbi: enabled
  config:
    pcrf:
      enabled: false
    pcf:
      enabled: true
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: kube-prometheus-stack

sgwc:
  enabled: false

sgwu:
  enabled: false

# AMF is enabled and gets the SBI label
amf:
  enabled: true
  podLabels:
    sbi: enabled
  config:
    guamiList:
      - plmn_id:
          mcc: &#34;999&#34;
          mnc: &#34;70&#34;
        amf_id:
          region: 2
          set: 1
    taiList:
      - plmn_id:
          mcc: &#34;999&#34;
          mnc: &#34;70&#34;
        tac: [1]
    plmnList:
      - plmn_id:
          mcc: &#34;999&#34;
          mnc: &#34;70&#34;
        s_nssai:
          - sst: 1
            sd: &#34;0x111111&#34;
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: kube-prometheus-stack

# UPF is enabled and gets the SBI label
upf:
  enabled: true
  podLabels:
    sbi: enabled
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: kube-prometheus-stack

# NSSF is enabled and gets the SBI label
nssf:
  enabled: true
  podLabels:
    sbi: enabled
  config:
    nsiList:
      - uri: &#34;&#34;
        sst: 1
        sd: &#34;0x111111&#34;
        
# Adding other default components to ensure they also get the label
nrf:
  enabled: true
  podLabels:
    sbi: enabled

ausf:
  enabled: true
  podLabels:
    sbi: enabled

udm:
  enabled: true
  podLabels:
    sbi: enabled

udr:
  enabled: true
  podLabels:
    sbi: enabled

bsf:
  enabled: true
  podLabels:
    sbi: enabled

webui:
  ingress:
    enabled: false

populate:
  enabled: true
  initCommands:
  - open5gs-dbctl add_ue_with_slice 999700000000001 465B5CE8B199B49FAA5F0A2EE238A6BC E8ED289DEBA952E4283B54E88E6183CA internet 1 111111
  - open5gs-dbctl add_ue_with_slice 999700000000002 465B5CE8B199B49FAA5F0A2EE238A6BC E8ED289DEBA952E4283B54E88E6183CA internet 1 111111

mongodb:
  enabled: true
  auth:
    enabled: false
  readinessProbe:
    initialDelaySeconds: 60
    periodSeconds: 20
    timeoutSeconds: 15
    failureThreshold: 6
    successThreshold: 1
  livenessProbe:
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 15
    failureThreshold: 6
  persistence:
    enabled: true
    size: 2Gi
    storageClass: &#34;open5gs-mongodb-pv&#34;
```
nano nano pv-mongodb.yaml
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: open5gs-mongodb-pv # You can choose a unique name for your PV
spec:
  capacity:
    storage: 2Gi # Must be equal to or greater than the PVC request
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce # MongoDB typically requires RWO
  persistentVolumeReclaimPolicy: Retain # Or Delete, depending on your needs
  storageClassName: open5gs-mongodb-pv
  hostPath:
    path: /mnt/data/mongodb # The directory you created on your node
```
nano sc-open5gs-mongodb.yaml:
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: open5gs-mongodb-pv # This name must match what the PVC and PV are using
provisioner: kubernetes.io/no-provisioner # Indicates PVs must be manually created
volumeBindingMode: WaitForFirstConsumer   # Good for local storage like hostPath
```
ke node mongodb terus:
```
sudo chown -R 1001:1001 /mnt/data/mongodb
sudo chmod -R 770 /mnt/data/mongodb
```

2. install lewat helm &amp; bikin namespace
```
kubectl create namespace open5gs

kubectl apply -f pv-mongodb.yaml
kubectl apply -f sc-open5gs-mongodb.yaml

helm install open5gs oci://registry-1.docker.io/gradiantcharts/open5gs --version 2.2.9 --values open5gs.yaml -n open5gs
```

UNINSTALL:
```
helm uninstall open5gs -n open5gs
kubectl delete pv open5gs-mongodb-pv
kubectl delete sc open5gs-mongodb-pv
```

ssh ke node mongodb:
```
sudo rm -rf /mnt/data/mongodb/*

sudo mkdir -p /mnt/data/mongodb
sudo chown -R 1001:1001 /mnt/data/mongodb
sudo chmod -R 770 /mnt/data/mongodb
```

## UERANSIM

3. buat yaml ueransim
nano ueransim.yaml
```
amf:
  hostname: &#34;open5gs-amf-ngap.open5gs.svc.cluster.local&#34;
  ip: null

mcc: &#39;999&#39;
mnc: &#39;70&#39;
sst: 1
sd: &#34;0x111111&#34;
tac: &#39;0001&#39;

ues:
  enabled: true
  count: 2
  initialMSISDN: &#39;0000000001&#39;
```
nano l4-ingress-amf-from-gnb-sctp.yaml
```                                            
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-ingress-from-gnb-sctp&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      &#34;k8s:app.kubernetes.io/name&#34;: amf 
  ingress:
  - fromEndpoints:
    - matchLabels:
        &#34;k8s:app.kubernetes.io/name&#34;: ueransim-gnb
        &#34;k8s:io.kubernetes.pod.namespace&#34;: ueransim
    toPorts:
    - ports:
      - port: &#34;38412&#34;
        protocol: SCTP
```
4. jalanin
```
kubectl create namespace ueransim
kubectl label namespace ueransim name=ueransim

kubectl apply -f l4-ingress-amf-from-gnb-sctp.yaml
kubectl patch configmap -n kube-system cilium-config --type merge -p &#39;{&#34;data&#34;:{&#34;enable-sctp&#34;:&#34;true&#34;}}&#39;

helm install ueransim-gnb oci://registry-1.docker.io/gradiant/ueransim-gnb --version 0.2.6 --values ueransim.yaml -n ueransim
```

3. akses hubble ui
nyalain
```
cilium hubble ui
```
ssh pake cmd
```
ssh -L 12000:localhost:12000 &lt;USER&gt;@&lt;IP&gt;
```

## Cilium
1. Allow port utk komunikasi smf-upf &amp; gnb-upf &amp; prometheus-amf

nano l4-egress-smf-to-upf.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-egress-smf-to-core&#34; # Or your chosen name
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      &#34;k8s:app.kubernetes.io/name&#34;: smf
  egress:
  # Rule for CoreDNS
  - toEndpoints:
    - matchLabels:
        &#34;k8s:io.kubernetes.pod.namespace&#34;: kube-system
        &#34;k8s:k8s-app&#34;: kube-dns
    toPorts:
    - ports:
      - port: &#34;53&#34;
        protocol: UDP
      rules: { dns: [{ matchPattern: &#34;*&#34; }] }
    - ports:
      - port: &#34;53&#34;
        protocol: TCP
      rules: { dns: [{ matchPattern: &#34;*&#34; }] }
  # Rule for UPF (PFCP N4 interface)
  - toEndpoints:
    - matchLabels:
        &#34;k8s:app.kubernetes.io/name&#34;: upf
        &#34;k8s:io.kubernetes.pod.namespace&#34;: open5gs
    toPorts:
    - ports:
      - port: &#34;8805&#34;
        protocol: UDP
  # Rule for NRF (SBI)
  - toEndpoints:
    - matchLabels:
        &#34;k8s:app.kubernetes.io/name&#34;: nrf
        &#34;k8s:io.kubernetes.pod.namespace&#34;: open5gs
    toPorts:
    - ports:
      - port: &#34;7777&#34;
        protocol: TCP
  # Rule for PCF (SBI)
  - toEndpoints:
    - matchLabels:
        &#34;k8s:app.kubernetes.io/name&#34;: pcf
        &#34;k8s:io.kubernetes.pod.namespace&#34;: open5gs
    toPorts:
    - ports:
      - port: &#34;7777&#34;
        protocol: TCP
  # Rule for UDM (SBI)
  - toEndpoints:
    - matchLabels:
        &#34;k8s:app.kubernetes.io/name&#34;: udm
        &#34;k8s:io.kubernetes.pod.namespace&#34;: open5gs
    toPorts:
    - ports:
      - port: &#34;7777&#34;
        protocol: TCP
  # Rule for AMF (SBI)
  - toEndpoints:
    - matchLabels:
        &#34;k8s:app.kubernetes.io/name&#34;: amf
        &#34;k8s:io.kubernetes.pod.namespace&#34;: open5gs
    toPorts:
    - ports:
      - port: &#34;7777&#34;
        protocol: TCP
  # --- ADD THIS RULE FOR SCP ---
  - toEndpoints:
    - matchLabels:
        &#34;k8s:app.kubernetes.io/name&#34;: scp
        &#34;k8s:io.kubernetes.pod.namespace&#34;: open5gs
    toPorts:
    - ports:
      - port: &#34;7777&#34;
        protocol: TCP
  # --- END OF SCP RULE ---
```
nano l4-ingress-upf-from-smf.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-ingress-upf-from-smf&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: upf
  ingress:
    - fromEndpoints:
        - matchLabels:
            app.kubernetes.io/name: smf
      toPorts:
        # For SMF-UPF internet traffic
        - ports:
            - port: &#34;8805&#34;
              protocol: UDP
```
nano l4-ingress-upf-from-gnb.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-ingress-upf-from-gnb&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: upf
  ingress:
    - fromEndpoints:
        - matchLabels:
            app.kubernetes.io/component: gnb
      toPorts:
        # For SMF-UPF internet traffic
        - ports:
            - port: &#34;2152&#34;
              protocol: UDP
```
nano l3-egress-upf-to-any.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l3-egress-upf-to-any&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: upf
  egress:
    - toCIDRSet:
        - cidr: 0.0.0.0/0
          # except private CIDR
          except:
            - 10.0.0.0/8
            - 172.16.0.0/12
            - 192.168.0.0/16
```
nano allow-prometheus-to-amf.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-prometheus-to-amf-metrics&#34;
  namespace: open5gs # Policy applied in the AMF&#39;s namespace
spec:
  endpointSelector:
    matchLabels:
      &#34;k8s:app.kubernetes.io/name&#34;: amf # Selects the AMF pods
  ingress:
  - fromEndpoints:
    - matchLabels:
        # Assuming Prometheus pods have these labels in the &#39;monitoring&#39; namespace
        # You&#39;ll need to verify the exact labels on your Prometheus pods
        &#34;k8s:io.kubernetes.pod.namespace&#34;: monitoring
        &#34;k8s:app.kubernetes.io/name&#34;: prometheus # Or &#34;kube-prometheus-stack-prometheus&#34;, etc.
        # Check your Prometheus pod labels with:
        # kubectl get pods -n monitoring -l app.kubernetes.io/instance=kube-prometheus-stack --show-labels
    toPorts:
    - ports:
      - port: &#34;9090&#34;
        protocol: TCP
```
nano l7-egress-scp-to-nrf.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  # The name of the policy object in Kubernetes
  name: &#34;l7-egress-scp-to-nrf&#34;
  # The namespace where your Open5GS components are running
  namespace: open5gs
spec:
  # This policy applies to the SCP pod(s)
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: scp

  # This rule defines outgoing (egress) traffic that is allowed
  egress:
  - toEndpoints:
    # The destination is the NRF pod(s)
    - matchLabels:
        app.kubernetes.io/name: nrf
    toPorts:
    - ports:
      # The SBI port for 5G core communication
      - port: &#34;7777&#34;
        protocol: TCP
```
nano l4-egress-to-mongodb.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  # The name of the policy object in Kubernetes
  name: &#34;l4-egress-to-mongodb&#34;
  # The namespace where your Open5GS components are running
  namespace: open5gs
spec:
  # This policy applies to all pods with the label &#39;app.kubernetes.io/instance: open5gs&#39;.
  # This is a common label for all the Open5GS core components in your deployment.
  endpointSelector:
    matchLabels:
      app.kubernetes.io/instance: open5gs

  # This defines an outgoing (egress) rule for the selected pods.
  egress:
  - toEndpoints:
    # The destination is the MongoDB pod(s).
    - matchLabels:
        app.kubernetes.io/name: mongodb
    toPorts:
    - ports:
      # The standard port for MongoDB.
      - port: &#34;27017&#34;
        protocol: TCP
```
apply:
```
kubectl apply -f l4-egress-smf-to-upf.yaml
kubectl apply -f l4-ingress-upf-from-smf.yaml
kubectl apply -f l4-ingress-upf-from-gnb.yaml
kubectl apply -f l3-egress-upf-to-any.yaml
kubectl apply -f allow-prometheus-to-amf.yaml
kubectl apply -f l7-egress-scp-to-nrf.yaml
kubectl apply -f l4-egress-to-mongodb.yaml

```
2. allow port mongodb
nano l4-egress-populate-webui-udr-pcf-to-mongodb.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-egress-populate-to-mongodb&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/component: populate
  egress:
    - toEndpoints:
        - matchLabels:
            app.kubernetes.io/name: mongodb
      toPorts:
        # For mongodb access
        - ports:
            - port: &#34;27017&#34;
              protocol: TCP
---
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-egress-webui-to-mongodb&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: webui
  egress:
    - toEndpoints:
        - matchLabels:
            app.kubernetes.io/name: mongodb
      toPorts:
        # For mongodb access
        - ports:
            - port: &#34;27017&#34;
              protocol: TCP
---
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-egress-udr-to-mongodb&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: udr
  egress:
    - toEndpoints:
        - matchLabels:
            app.kubernetes.io/name: mongodb
      toPorts:
        # For mongodb access
        - ports:
            - port: &#34;27017&#34;
              protocol: TCP
---
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-egress-pcf-to-mongodb&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: pcf
  egress:
    - toEndpoints:
        - matchLabels:
            app.kubernetes.io/name: mongodb
      toPorts:
        # For mongodb access
        - ports:
            - port: &#34;27017&#34;
              protocol: TCP
```
nano l7-egress-to-scp.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l7-egress-to-scp&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      sbi: enabled
  egress: 
    - toEndpoints:
        - matchLabels:
            app.kubernetes.io/name: scp
      toPorts:
        # For SBI communication
        - ports:
            - port: &#34;7777&#34;
              protocol: TCP
          rules:
            http: [{}]

```
nano l7-egress-scp-to-core.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l7-egress-scp-to-core&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: scp
  egress: 
    - toEndpoints:
        - matchLabels:
            sbi: enabled
      toPorts:
        # For SBI communication
        - ports:
            - port: &#34;7777&#34;
              protocol: TCP
          rules:
            http: [{}]

```
nano l7-ingress-from-scp.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l7-ingress-from-scp&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      sbi: enabled
  ingress: 
    - fromEndpoints:
        - matchLabels:
            app.kubernetes.io/name: scp
        - matchLabels:
            sbi: enabled
      toPorts:
        # For SBI communication
        - ports:
            - port: &#34;7777&#34;
              protocol: TCP
          rules:
            http: [{}]

```
nano allow-prometheus-to-upf.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-prometheus-to-upf-metrics&#34;
  namespace: open5gs # Policy created in the UPF&#39;s namespace
spec:
  endpointSelector:
    matchLabels:
      # Selects your UPF pod(s)
      # Based on previous info, UPF pods have app.kubernetes.io/name=upf
      &#34;k8s:app.kubernetes.io/name&#34;: upf
  ingress:
  - fromEndpoints:
    - matchLabels:
        # Selects your Prometheus pod(s) in the &#39;monitoring&#39; namespace
        &#34;k8s:io.kubernetes.pod.namespace&#34;: monitoring
        &#34;k8s:app.kubernetes.io/name&#34;: prometheus # This label was present on your Prometheus pod
    toPorts:
    - ports:
      - port: &#34;9090&#34;
        protocol: TCP
```
nano allow-scp-to-amf-sbi.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-scp-to-amf-sbi&#34;
  namespace: open5gs # Policy in AMF&#39;s namespace
spec:
  endpointSelector:
    matchLabels:
      &#34;k8s:app.kubernetes.io/name&#34;: amf # Selects AMF pods
  ingress:
  - fromEndpoints:
    - matchLabels:
        &#34;k8s:app.kubernetes.io/name&#34;: scp # Selects SCP pods
        &#34;k8s:io.kubernetes.pod.namespace&#34;: open5gs
    toPorts:
    - ports:
      - port: &#34;7777&#34;
        protocol: TCP
```
apply:
```
kubectl apply -f l7-egress-to-scp.yaml 
kubectl apply -f l7-ingress-from-scp.yaml
kubectl apply -f l7-egress-scp-to-nrf.yaml
kubectl apply -f l4-egress-populate-webui-udr-pcf-to-mongodb.yaml
kubectl apply -f allow-prometheus-to-upf.yaml
kubectl apply -f allow-scp-to-amf-sbi.yaml
```

3. ga ngerti lagi
nano allow-upf-gtpu-egress.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-open5gs-upf-gtpu-egress&#34;
  namespace: open5gs # Ensure this is the namespace of your UPF pods
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: upf # Selects your Open5GS UPF pods
      # You can add app.kubernetes.io/instance: open5gs if needed for more specificity
      # app.kubernetes.io/instance: open5gs 
  egress:
  - toCIDR:
    - &#34;10.0.0.98/32&#34;   # The destination IP address for GTP-U traffic
    toPorts:
    - ports:
      - port: &#34;8805&#34;
        protocol: UDP
```
nano allow-smf-diameter-egress.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-open5gs-smf-diameter-egress&#34;
  namespace: open5gs # Ensure this is the namespace of your SMF pods
spec:
  endpointSelector:
    matchLabels:
      app.kubernetes.io/name: smf # Selects your Open5GS SMF pods
      # You can add app.kubernetes.io/instance: open5gs if needed for more specificity
      # app.kubernetes.io/instance: open5gs
  egress:
  - toCIDR:
    - &#34;10.105.251.24/32&#34; # The destination IP for Diameter (e.g., PCF/CHF)
    toPorts:
    - ports:
      - port: &#34;3868&#34;
        protocol: TCP
```
nano allow-pfcp-policy.yaml
```
# This file contains two policies to allow PFCP communication between SMF and UPF.

# Policy 1: Allow SMF to send PFCP traffic to UPF (Egress from SMF)
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-smf-to-upf-pfcp-egress&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      # Selects the Open5GS SMF pods
      app.kubernetes.io/name: smf
  egress:
  - toEndpoints:
    # Selects the Open5GS UPF pods as the destination
    - matchLabels:
        app.kubernetes.io/name: upf
    toPorts:
    - ports:
      - port: &#34;8805&#34; # Standard PFCP port
        protocol: UDP

---

# Policy 2: Allow UPF to receive PFCP traffic from SMF (Ingress to UPF)
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-pfcp-from-smf-to-upf-ingress&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      # Selects the Open5GS UPF pods
      app.kubernetes.io/name: upf
  ingress:
  - fromEndpoints:
    # Selects the Open5GS SMF pods as the source
    - matchLabels:
        app.kubernetes.io/name: smf
    toPorts:
    - ports:
      - port: &#34;8805&#34; # Standard PFCP port
        protocol: UDP
```
nano allow-dns-policy.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-dns-lookup&#34;
  namespace: open5gs # This policy will apply to the open5gs namespace
spec:
  # This selects all pods in the namespace. You could make it more specific
  # by adding a selector for the SMF pod if you wish.
  endpointSelector: {}
  egress:
  - toEndpoints:
    - matchLabels:
        # This standard label selects the Kubernetes DNS service endpoint
        k8s:io.kubernetes.pod.namespace: kube-system
        k8s-app: kube-dns
    toPorts:
    - ports:
      - port: &#34;53&#34;
        protocol: UDP
      # Optional but recommended: also allow DNS over TCP
      rules:
        dns:
        - matchPattern: &#34;*&#34;
    - ports:
      - port: &#34;53&#34;
        protocol: TCP
      rules:
        dns:
        - matchPattern: &#34;*&#34;
```
nano allow-amf-sbi.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-amf-sbi-egress&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      # This policy applies to the AMF pod
      app.kubernetes.io/name: amf
  egress:
  # This rule allows the AMF to talk to the AUSF, UDM, and SCP
  - toEndpoints:
    - matchLabels:
        # Allow to AUSF
        app.kubernetes.io/name: ausf
    - matchLabels:
        # Allow to UDM
        app.kubernetes.io/name: udm
    - matchLabels:
        # Allow to SCP (Service Communication Proxy)
        app.kubernetes.io/name: scp
    toPorts:
    - ports:
      - port: &#34;7777&#34; # The standard SBI port
        protocol: TCP
```
nano allow-scp-sbi.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-scp-sbi-egress&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      # This policy applies to the Service Communication Proxy (SCP)
      app.kubernetes.io/name: scp
  egress:
  # This rule allows the SCP to talk to all other core NFs
  - toEndpoints:
    - matchLabels:
        app.kubernetes.io/name: nrf
    - matchLabels:
        app.kubernetes.io/name: ausf
    - matchLabels:
        app.kubernetes.io/name: udm
    - matchLabels:
        app.kubernetes.io/name: pcf
    - matchLabels:
        app.kubernetes.io/name: nssf
    - matchLabels:
        app.kubernetes.io/name: smf
    toPorts:
    - ports:
      - port: &#34;7777&#34; # The standard SBI port
        protocol: TCP
```
nano allow-ausf-sbi.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-ausf-sbi-egress&#34;
  namespace: open5gs
spec:
  endpointSelector:
    matchLabels:
      # This policy applies to the AUSF pod
      app.kubernetes.io/name: ausf
  egress:
  # This rule allows the AUSF to send its registration to the SCP
  - toEndpoints:
    - matchLabels:
        # Allow to SCP
        app.kubernetes.io/name: scp
    toPorts:
    - ports:
      - port: &#34;7777&#34;
        protocol: TCP
```
nano allow-all-internal.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-all-open5gs-internal-sbi&#34;
  namespace: open5gs
spec:
  # This policy applies to ALL pods in the open5gs namespace
  endpointSelector: {}
  # Allow them to send traffic to any other pod in the same namespace on the SBI port
  egress:
  - toEndpoints:
    - matchLabels:
        # any pod in the open5gs namespace
        &#34;k8s:io.kubernetes.pod.namespace&#34;: open5gs
    toPorts:
    - ports:
      - port: &#34;7777&#34;
        protocol: TCP
```
nano allow-open5gs-dns.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;l4-egress-to-dns&#34; # You can rename this if you like
  namespace: open5gs
spec:
  # This selector applies the policy to all pods in the namespace
  endpointSelector: {}
  egress:
  - toEndpoints:
    - matchLabels:
        # This standard label selects the Kubernetes DNS service
        k8s:io.kubernetes.pod.namespace: kube-system
        k8s-app: kube-dns
    toPorts:
    - ports:
      - port: &#34;53&#34;
        protocol: UDP
      rules:
        dns:
        - matchPattern: &#34;*&#34;
    - ports:
      - port: &#34;53&#34;
        protocol: TCP
      rules:
        dns:
        - matchPattern: &#34;*&#34;
```
nano allow-all-internal-sbi.yaml
```
apiVersion: &#34;cilium.io/v2&#34;
kind: CiliumNetworkPolicy
metadata:
  name: &#34;allow-all-open5gs-internal-sbi&#34;
  namespace: open5gs
spec:
  # This policy selects EVERY pod in the open5gs namespace
  endpointSelector: {}
  
  # This rule allows them to send traffic TO any other pod
  # in the same namespace on the main communication port (7777)
  egress:
  - toEndpoints:
    - matchLabels:
        # The key &#34;k8s:io.kubernetes.pod.namespace&#34; is a special label
        # that Cilium adds to identify pods in a namespace.
        &#34;k8s:io.kubernetes.pod.namespace&#34;: open5gs
    toPorts:
    - ports:
      - port: &#34;7777&#34;
        protocol: TCP
```
apply:
```
kubectl apply -f allow-smf-diameter-egress.yaml
kubectl apply -f allow-upf-gtpu-egress.yaml
kubectl apply -f allow-pfcp-policy.yaml
kubectl apply -f allow-dns-policy.yaml
kubectl apply -f allow-amf-sbi.yaml
kubectl apply -f allow-scp-sbi.yaml
kubectl apply -f allow-ausf-sbi.yaml
kubectl apply -f allow-open5gs-dns.yaml
```
